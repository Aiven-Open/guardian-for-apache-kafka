package io.aiven.guardian.kafka.backup.s3

import akka.Done
import akka.NotUsed
import akka.actor.ActorSystem
import akka.kafka.ConsumerSettings
import akka.kafka.ProducerSettings
import akka.kafka.scaladsl.Producer
import akka.stream.KillSwitches
import akka.stream.SharedKillSwitch
import akka.stream.alpakka.s3.ListBucketResultContents
import akka.stream.alpakka.s3.S3Settings
import akka.stream.alpakka.s3.scaladsl.S3
import akka.stream.scaladsl.Sink
import akka.stream.scaladsl.Source
import com.softwaremill.diffx.scalatest.DiffMatcher.matchTo
import io.aiven.guardian.akka.AnyPropTestKit
import io.aiven.guardian.kafka.Generators.KafkaDataInChunksWithTimePeriod
import io.aiven.guardian.kafka.Generators.kafkaDataWithMinSizeGen
import io.aiven.guardian.kafka.KafkaClient
import io.aiven.guardian.kafka.KafkaClusterTest
import io.aiven.guardian.kafka.Utils._
import io.aiven.guardian.kafka.backup.configs.Backup
import io.aiven.guardian.kafka.backup.configs.ChronoUnitSlice
import io.aiven.guardian.kafka.backup.configs.PeriodFromFirst
import io.aiven.guardian.kafka.codecs.Circe._
import io.aiven.guardian.kafka.configs.KafkaCluster
import io.aiven.guardian.kafka.models.ReducedConsumerRecord
import io.aiven.guardian.kafka.s3.Generators.s3ConfigGen
import io.aiven.guardian.kafka.s3.configs.{S3 => S3Config}
import org.apache.kafka.clients.CommonClientConfigs
import org.apache.kafka.clients.admin.AdminClient
import org.apache.kafka.clients.admin.NewTopic
import org.apache.kafka.clients.producer.ProducerRecord
import org.apache.kafka.common.serialization.ByteArraySerializer
import org.mdedetrich.akka.stream.support.CirceStreamSupport

import scala.concurrent.Future
import scala.concurrent.duration._
import scala.jdk.CollectionConverters._
import scala.jdk.FutureConverters._
import scala.language.postfixOps

import java.time.temporal.ChronoUnit
import java.util.Base64

class RealS3BackupClientSpec
    extends AnyPropTestKit(ActorSystem("RealS3BackupClientSpec"))
    with BackupClientSpec
    with KafkaClusterTest {
  override lazy val s3Settings: S3Settings = S3Settings()

  /** Virtual Dot Host in bucket names are disabled because you need an actual DNS certificate otherwise AWS will fail
    * on bucket creation
    */
  override lazy val useVirtualDotHost: Boolean            = false
  override lazy val bucketPrefix: Option[String]          = Some("guardian-")
  override lazy val enableCleanup: Option[FiniteDuration] = Some(5 seconds)

  /** Timeout constant to wait for both Akka Streams plus initialization of consumer/kafka cluster
    */
  val KafkaInitializationTimeoutConstant: FiniteDuration = AkkaStreamInitializationConstant + (2.5 seconds)

  case object TerminationException extends Exception("termination-exception")

  def reducedConsumerRecordsToJson(reducedConsumerRecords: List[ReducedConsumerRecord]): Array[Byte] = {
    import io.aiven.guardian.kafka.codecs.Circe._
    import io.circe.syntax._
    reducedConsumerRecords.asJson.noSpaces.getBytes
  }

  def baseKafkaConfig: Some[ConsumerSettings[Array[Byte], Array[Byte]] => ConsumerSettings[Array[Byte], Array[Byte]]] =
    Some(
      _.withBootstrapServers(
        container.bootstrapServers
      ).withGroupId("test-group")
    )

  def createKafkaClient(
      killSwitch: SharedKillSwitch
  )(implicit kafkaClusterConfig: KafkaCluster): KafkaClientWithKillSwitch =
    new KafkaClientWithKillSwitch(
      configureConsumer = baseKafkaConfig,
      killSwitch = killSwitch
    )

  /** Converts a generated list of `ReducedConsumerRecord` to a list of `ProducerRecord`
    * @param data
    *   List of `ReducedConsumerRecord`'s generated by scalacheck
    * @return
    *   A list of `ProducerRecord`. Note that it only uses the `key`/`value` and ignores other values
    */
  def toProducerRecords(data: List[ReducedConsumerRecord]): List[ProducerRecord[Array[Byte], Array[Byte]]] = data.map {
    reducedConsumerRecord =>
      val keyAsByteArray   = Base64.getDecoder.decode(reducedConsumerRecord.key)
      val valueAsByteArray = Base64.getDecoder.decode(reducedConsumerRecord.value)
      new ProducerRecord[Array[Byte], Array[Byte]](reducedConsumerRecord.topic, keyAsByteArray, valueAsByteArray)
  }

  /** Converts a list of `ProducerRecord` to a source that is streamed over a period of time
    * @param producerRecords
    *   The list of producer records
    * @param streamDuration
    *   The period over which the topics will be streamed
    * @return
    *   Source ready to be passed into a Kafka producer
    */
  def toSource(
      producerRecords: List[ProducerRecord[Array[Byte], Array[Byte]]],
      streamDuration: FiniteDuration
  ): Source[ProducerRecord[Array[Byte], Array[Byte]], NotUsed] = {
    val durationToMicros = streamDuration.toMillis
    val topicsPerMillis  = producerRecords.size / durationToMicros
    Source(producerRecords).throttle(topicsPerMillis.toInt, 1 millis)
  }

  /** Call this function to send a message after the next step of configured time period to trigger a rollover so the
    * current object will finish processing
    * @param duration
    * @param producerSettings
    * @param topic
    * @return
    */
  def sendTopicAfterTimePeriod(duration: FiniteDuration,
                               producerSettings: ProducerSettings[Array[Byte], Array[Byte]],
                               topic: String
  ): Future[Done] = akka.pattern.after(duration) {
    Source(
      List(
        new ProducerRecord[Array[Byte], Array[Byte]](topic, "1".getBytes, "1".getBytes)
      )
    ).runWith(Producer.plainSink(producerSettings))
  }

  def createProducer(): ProducerSettings[Array[Byte], Array[Byte]] =
    ProducerSettings(system, new ByteArraySerializer, new ByteArraySerializer)
      .withBootstrapServers(container.bootstrapServers)

  case class DownloadNotReady(downloads: Seq[ListBucketResultContents])
      extends Exception(s"Download not ready, current state is ${downloads.map(_.toString).mkString(",")}")

  def getKeyFromSingleDownload(dataBucket: String): Future[String] = waitForS3Download(
    dataBucket,
    {
      case Seq(single) => single.key
      case rest =>
        throw DownloadNotReady(rest)
    }
  )

  property("entire flow works properly from start to end") {
    forAll(kafkaDataWithMinSizeGen(S3.MinChunkSize, 2, reducedConsumerRecordsToJson),
           s3ConfigGen(useVirtualDotHost, bucketPrefix)
    ) { (kafkaDataInChunksWithTimePeriod: KafkaDataInChunksWithTimePeriod, s3Config: S3Config) =>
      logger.info(s"Data bucket is ${s3Config.dataBucket}")

      val data = kafkaDataInChunksWithTimePeriod.data.flatten

      val topics = data.map(_.topic).toSet

      val asProducerRecords = toProducerRecords(data)
      val baseSource        = toSource(asProducerRecords, 30 seconds)

      implicit val kafkaClusterConfig: KafkaCluster = KafkaCluster(topics)

      implicit val config: S3Config     = s3Config
      implicit val backupConfig: Backup = Backup(PeriodFromFirst(1 minute))

      val producerSettings = createProducer()

      val backupClient =
        new BackupClient(Some(s3Settings))(new KafkaClient(configureConsumer = baseKafkaConfig),
                                           implicitly,
                                           implicitly,
                                           implicitly,
                                           implicitly
        )

      val adminClient = AdminClient.create(
        Map[String, AnyRef](
          CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG -> container.bootstrapServers
        ).asJava
      )

      val createTopics = adminClient.createTopics(topics.map { topic =>
        new NewTopic(topic, 1, 1.toShort)
      }.asJava)

      val calculatedFuture = for {
        _ <- createTopics.all().toCompletableFuture.asScala
        _ <- createBucket(s3Config.dataBucket)
        _ = backupClient.backup.run()
        _ <- akka.pattern.after(KafkaInitializationTimeoutConstant)(
               baseSource
                 .runWith(Producer.plainSink(producerSettings))
             )
        _   <- sendTopicAfterTimePeriod(1 minute, producerSettings, topics.head)
        key <- getKeyFromSingleDownload(s3Config.dataBucket)
        downloaded <-
          S3.download(s3Config.dataBucket, key)
            .withAttributes(s3Attrs)
            .runWith(Sink.head)
            .flatMap {
              case Some((downloadSource, _)) =>
                downloadSource.via(CirceStreamSupport.decode[List[Option[ReducedConsumerRecord]]]).runWith(Sink.seq)
              case None => throw new Exception(s"Expected object in bucket ${s3Config.dataBucket} with key $key")
            }

      } yield downloaded.toList.flatten.collect { case Some(reducedConsumerRecord) =>
        reducedConsumerRecord
      }

      val downloaded = calculatedFuture.futureValue

      val downloadedGroupedAsKey = downloaded
        .groupBy(_.key)
        .view
        .mapValues { reducedConsumerRecords =>
          reducedConsumerRecords.map(_.value)
        }
        .toMap

      val inputAsKey = data
        .groupBy(_.key)
        .view
        .mapValues { reducedConsumerRecords =>
          reducedConsumerRecords.map(_.value)
        }
        .toMap

      downloadedGroupedAsKey must matchTo(inputAsKey)
    }
  }

  def waitUntilBackupClientHasCommitted(backupClient: BackupClientChunkState[_],
                                        step: FiniteDuration = 100 millis,
                                        delay: FiniteDuration = 5 seconds
  ): Future[Unit] =
    if (backupClient.processedChunks.size() > 0)
      akka.pattern.after(delay)(Future.successful(()))
    else
      akka.pattern.after(step)(waitUntilBackupClientHasCommitted(backupClient, step, delay))

  property("suspend/resume works correctly") {
    forAll(kafkaDataWithMinSizeGen(S3.MinChunkSize, 2, reducedConsumerRecordsToJson),
           s3ConfigGen(useVirtualDotHost, bucketPrefix)
    ) { (kafkaDataInChunksWithTimePeriod: KafkaDataInChunksWithTimePeriod, s3Config: S3Config) =>
      logger.info(s"Data bucket is ${s3Config.dataBucket}")

      val data = kafkaDataInChunksWithTimePeriod.data.flatten

      val topics = data.map(_.topic).toSet

      implicit val kafkaClusterConfig: KafkaCluster = KafkaCluster(topics)

      implicit val config: S3Config     = s3Config
      implicit val backupConfig: Backup = Backup(ChronoUnitSlice(ChronoUnit.MINUTES))

      val producerSettings = createProducer()

      val firstKillSwitch = KillSwitches.shared("first-kill-switch")

      val backupClient =
        new BackupClientChunkState(Some(s3Settings))(createKafkaClient(firstKillSwitch),
                                                     implicitly,
                                                     implicitly,
                                                     implicitly,
                                                     implicitly
        )

      val asProducerRecords = toProducerRecords(data)
      val baseSource        = toSource(asProducerRecords, 30 seconds)

      val adminClient = AdminClient.create(
        Map[String, AnyRef](
          CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG -> container.bootstrapServers
        ).asJava
      )

      val createTopics = adminClient.createTopics(topics.map { topic =>
        new NewTopic(topic, 1, 1.toShort)
      }.asJava)

      val calculatedFuture = for {
        _ <- createTopics.all().toCompletableFuture.asScala
        _ <- createBucket(s3Config.dataBucket)
        _ = backupClient.backup.run()
        _ <- waitForStartOfTimeUnit(ChronoUnit.MINUTES)
        _ = baseSource.runWith(Producer.plainSink(producerSettings))
        _ <- waitUntilBackupClientHasCommitted(backupClient)
        _ = firstKillSwitch.abort(TerminationException)
        secondBackupClient <- akka.pattern.after(2 seconds) {
                                Future {
                                  new BackupClient(Some(s3Settings))(
                                    new KafkaClient(configureConsumer = baseKafkaConfig),
                                    implicitly,
                                    implicitly,
                                    implicitly,
                                    implicitly
                                  )
                                }
                              }
        _ = secondBackupClient.backup.run()
        _   <- sendTopicAfterTimePeriod(1 minutes, producerSettings, topics.head)
        key <- getKeyFromSingleDownload(s3Config.dataBucket)
        downloaded <- S3.download(s3Config.dataBucket, key)
                        .withAttributes(s3Attrs)
                        .runWith(Sink.head)
                        .flatMap {
                          case Some((downloadSource, _)) =>
                            downloadSource
                              .via(CirceStreamSupport.decode[List[Option[ReducedConsumerRecord]]])
                              .runWith(Sink.seq)
                          case None =>
                            throw new Exception(s"Expected object in bucket ${s3Config.dataBucket} with key $key")
                        }

      } yield downloaded.toList.flatten.collect { case Some(reducedConsumerRecord) =>
        reducedConsumerRecord
      }

      val downloaded = calculatedFuture.futureValue

      // Only care about ordering when it comes to key
      val downloadedGroupedAsKey = downloaded
        .groupBy(_.key)
        .view
        .mapValues { reducedConsumerRecords =>
          reducedConsumerRecords.map(_.value)
        }
        .toMap

      val inputAsKey = data
        .groupBy(_.key)
        .view
        .mapValues { reducedConsumerRecords =>
          reducedConsumerRecords.map(_.value)
        }
        .toMap

      downloadedGroupedAsKey must matchTo(inputAsKey)
    }
  }
}
